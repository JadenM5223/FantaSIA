{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5SDHgsE6rRo"
   },
   "source": [
    "# 빌딩 검출\n",
    "\n",
    "## 파라미터 : \n",
    "*  epoch : 30\n",
    "* beta1: 0.9, beta2 : 0.999 \n",
    "* learning rate : 0.0001\n",
    "\n",
    "## 라벨링 방식 : 흑백 채우기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XdmkWGKEwEC9",
    "outputId": "461a5dc4-c91f-434f-8900-f34440ccf23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models in /home/ssac18/.local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: efficientnet==1.0.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: image-classifiers==1.0.0 in /home/ssac18/.local/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from segmentation_models) (1.0.8)\n",
      "Requirement already satisfied: scikit-image in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.2)\n",
      "Requirement already satisfied: h5py in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\n",
      "Requirement already satisfied: six in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.6.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.3.4)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (8.1.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.1.14)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ssac18/anaconda3/envs/aiffel/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJLJFYTKzf_3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJRJI_oUuJvC",
    "outputId": "c074ce87-59d1-4dcd-bd85-d63973303c3e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YQZ02khSvKeY"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "import re\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxUVBql6ul6x",
    "outputId": "27739122-1c20-4c22-c1e2-c1fc8b458e8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.3.0'\n",
      " 0603_roads_label01_v1.h5\n",
      "'[AIFFEL해커톤][SIA] 데이터 전달, 멘토링 방법_202105.pptx'\n",
      " best_model.h5\n",
      " best_model.pth\n",
      " binary_roads_imagepatch.ipynb\n",
      "'BnR 라벨링 전처리.ipynb'\n",
      " BnR_train_list.txt\n",
      " BnR_val_list.txt\n",
      "'Building Detection'\n",
      " buildings_eda.ipynb\n",
      "'cars segmentation (camvid).ipynb'\n",
      " data\n",
      " multi_bnr_augmentation.ipynb\n",
      "'multiclass segmentation (camvid)_원본.ipynb'\n",
      "'parsing BnR .ipynb'\n",
      " parsing_building.ipynb\n",
      " parsing_roads.ipynb\n",
      " 투명.png\n",
      " preprocessing\n",
      "'Road Detection'\n",
      " road_segmentation_v1.ipynb\n",
      " road_segmentation_v3.ipynb\n",
      "'Segmentation Models - building.ipynb'\n",
      "'Segmentation Models- pytorch.ipynb'\n",
      " SegNet\n",
      "'SIA 해커톤 과업지시서.pdf'\n",
      " UnetBuildingtrain.ipynb\n",
      "ls: cannot access '/content/drive/MyDrive/SIA/colab_code/lhk/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!ls /content/drive/MyDrive/SIA/colab_code/lhk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w4qp9j5ru95C"
   },
   "outputs": [],
   "source": [
    "DATA_DIR =  './data/buildings/train/'\n",
    "VAL_DIR = './data/buildings/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1bt4XW5Ou0Ry"
   },
   "outputs": [],
   "source": [
    "B_train_image_path = './data/buildings/train/raw_image/' #빌딩 트레인 input image 경로\n",
    "B_train_label_path = './data/buildings/train/new_label/' #빌딩 트레인 변환된 label 경로\n",
    "\n",
    "B_val_image_path = './data/buildings/val/raw_image/' #빌딩 validation input image 경로 \n",
    "B_val_label_path = './data/buildings/val/new_label/' #빌딩 validation 변환된 label image 경로\n",
    "\n",
    "R_train_image_path = './data/roads/train/raw_image/' #도로 트레인 input image 경로 \n",
    "R_train_label_path = './data/roads/train/new_label/' #도로 트레인 변환된 label 경로\n",
    "\n",
    "R_val_image_path = './data/roads/val/raw_image/' #도로 validation input image 경로\n",
    "R_val_label_path = './data/roads/val/new_label/' #도로 트레인 변환된 label 경로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gvCXHS0bvF4d"
   },
   "outputs": [],
   "source": [
    "BnR_train_png_files = glob.glob(B_train_image_path + '*.png')\n",
    "B_train_label_png_files = glob.glob(B_train_label_path + '*.png')\n",
    "R_train_label_png_files = glob.glob(R_train_label_path + '*.png')\n",
    "\n",
    "BnR_val_png_files = glob.glob(B_val_image_path + '*.png')\n",
    "B_val_label_png_files = glob.glob(B_val_label_path + '*.png')\n",
    "R_val_label_png_files = glob.glob(R_val_label_path + '*.png')\n",
    "\n",
    "R_train_png_files =  glob.glob(R_train_image_path + '*.png')\n",
    "R_val_png_files =   glob.glob(R_val_image_path + '*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "kbB5n4qFvi9X"
   },
   "outputs": [],
   "source": [
    "## lv2 데이터셋의 train과 validation은  기존(lv1)의 train, validation에서 혼합되어 나옴 \n",
    "\n",
    "## 그러므로 기존 train, validation 명단을 합쳐서 명단을 만들어냄 \n",
    "BnR_input_png_files = BnR_train_png_files + BnR_val_png_files # 빌딩은 BnR으로 그대로 쓰면됨 \n",
    "R_input_png_files = R_train_png_files + R_val_png_files\n",
    "\n",
    "B_label_png_files = B_train_label_png_files + B_val_label_png_files\n",
    "R_label_png_files = R_train_label_png_files + R_val_label_png_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2dVsnf1uwQA"
   },
   "source": [
    "# 전체 파일 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "HxbESxIeuwQA"
   },
   "outputs": [],
   "source": [
    "## 위에서 만든 전체 파일은 train, validation 파일이기때문에 파일의 경로들이 다 다름.\n",
    "## 이를 해결하기 위해서 파일명만 남기게 함수를 만들어줌 \n",
    "\n",
    "def file_name_only(files_list):\n",
    "    filename_only = []\n",
    "    for i in files_list:\n",
    "        filename_only.append(i.split('/')[-1])\n",
    "    return filename_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zlKGd8BFuwQB"
   },
   "outputs": [],
   "source": [
    "BnR_input_png_files_only = file_name_only(BnR_input_png_files)\n",
    "B_label_png_files_only =file_name_only(B_label_png_files)\n",
    "R_label_png_files_only = file_name_only(R_label_png_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnz1i3-VuwQB"
   },
   "source": [
    "## Lv2 에 쓸 파일 목록명단이랑 체크하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Woay6Rv5uwQB"
   },
   "outputs": [],
   "source": [
    "def check_lv2_list(lv2_list,png_filesname_only,file_path_included):\n",
    "    lv2 = []\n",
    "    \n",
    "    f = open(lv2_list,'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        a = line.split('.')[0] + '.png'\n",
    "        if a in png_filesname_only:\n",
    "            ind = png_filesname_only.index(a)\n",
    "            lv2.append(file_path_included[ind])\n",
    "    f.close()\n",
    "    \n",
    "    return lv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS-ascsauwQB"
   },
   "source": [
    "## Lv2 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "RpfOr18AuwQB"
   },
   "outputs": [],
   "source": [
    "BnR_train_input_list = check_lv2_list(\"./BnR_train_list.txt\",BnR_input_png_files_only,BnR_input_png_files)\n",
    "Bnr_train_label_list = check_lv2_list(\"./BnR_train_list.txt\",B_label_png_files_only,B_label_png_files)\n",
    "bnR_train_label_list = check_lv2_list(\"./BnR_train_list.txt\",R_label_png_files_only,R_label_png_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6QP21LkuwQC"
   },
   "source": [
    "## lv2 val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "hHBXuQxvuwQC"
   },
   "outputs": [],
   "source": [
    "BnR_val_input_list = check_lv2_list(\"./BnR_val_list.txt\",BnR_input_png_files_only,BnR_input_png_files)\n",
    "Bnr_val_label_list = check_lv2_list(\"./BnR_val_list.txt\",B_label_png_files_only,B_label_png_files)\n",
    "bnR_val_label_list = check_lv2_list(\"./BnR_val_list.txt\",R_label_png_files_only,R_label_png_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGFcpzh6uwQC"
   },
   "source": [
    "# 명단에 들어가는 파일만 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "mope9XPZuwQC"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "# helper function for data visualization    \n",
    "def denormalize(x):\n",
    "    \"\"\"Scale image to range 0..1 for correct plot\"\"\"\n",
    "    x_max = np.percentile(x, 98)\n",
    "    x_min = np.percentile(x, 2)    \n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    x = x.clip(0, 1)\n",
    "    return x\n",
    "    \n",
    "\n",
    "# classes for data loading and preprocessing\n",
    "class Dataset_multi:\n",
    "    CLASSES =  ['roads','buildings']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir_b, \n",
    "            masks_dir_r,\n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = [i.split('/')[-1] for i in images_dir]\n",
    "        self.images_fps = images_dir\n",
    "        self.masks_fps_b = masks_dir_b\n",
    "        self.masks_fps_r = masks_dir_r\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask_b = cv2.imread(self.masks_fps_b[i], 0)\n",
    "        mask_r = cv2.imread(self.masks_fps_r[i], 0)\n",
    "        mask_r = np.where(mask_b != mask_r, mask_r, 0)\n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "        mask = np.stack((mask_b,mask_r), axis=-1).astype('float')\n",
    "        \n",
    "        # extract certain classes from mask (e.g. cars)\n",
    "#         masks = [(mask == v) for v in self.class_values]\n",
    "#         mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        # add background if mask is not binary\n",
    "        if mask.shape[-1] != 1:\n",
    "            # 둘다 1일 경우..\n",
    "            background = 255 - mask.sum(axis=-1, keepdims=True)\n",
    "            mask = np.concatenate((mask, background), axis=-1)\n",
    "            mask = np.where(mask == 255, mask, 0)\n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "#         image = image/255         \n",
    "        mask = mask/255\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "i4lhncQwuwQF"
   },
   "outputs": [],
   "source": [
    "class Dataloder(keras.utils.Sequence):\n",
    "    \"\"\"Load data from dataset and form batches\n",
    "    \n",
    "    Args:\n",
    "        dataset: instance of Dataset class for image loading and preprocessing.\n",
    "        batch_size: Integet number of images in batch.\n",
    "        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, batch_size=1, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(dataset))\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # collect batch data\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            data.append(self.dataset[j])\n",
    "        \n",
    "        # transpose list of lists\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "       \n",
    "        return tuple(batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch\"\"\"\n",
    "        return len(self.indexes) // self.batch_size\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.indexes = np.random.permutation(self.indexes)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "ha2AJDxaa6tn"
   },
   "outputs": [],
   "source": [
    "\n",
    "from albumentations import (\n",
    "                            Compose,\n",
    "                            HorizontalFlip,\n",
    "                            ShiftScaleRotate,\n",
    "                            OneOf,\n",
    "                            RandomContrast,\n",
    "                            RandomGamma,\n",
    "                            RandomBrightness\n",
    "   \n",
    "                            )\n",
    "\n",
    "\n",
    "def augmentations(prob=0.5):\n",
    "    \n",
    "    transformer = Compose([\n",
    "                      \n",
    "            HorizontalFlip(p=prob),\n",
    "            ShiftScaleRotate(p=prob, shift_limit=0.1, scale_limit=.1, rotate_limit=10),\n",
    "            OneOf([RandomContrast(limit=0.1, p=prob),\n",
    "                   RandomGamma(gamma_limit=(90, 110), p=prob),\n",
    "                   RandomBrightness(limit=0.1, p=prob)],p=prob),\n",
    "            \n",
    "    ], p=prob)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-YZyJk2HuwQG"
   },
   "outputs": [],
   "source": [
    "# def get_validation_augmentation():\n",
    "#     \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "#     test_transform = [\n",
    "#         A.PadIfNeeded(1024, 1024)\n",
    "#     ]\n",
    "#     return A.Compose(test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "qQ1HUXrWuwQH"
   },
   "outputs": [],
   "source": [
    "# def get_preprocessing(preprocessing_fn):\n",
    "#     \"\"\"Construct preprocessing transform\n",
    "    \n",
    "#     Args:\n",
    "#         preprocessing_fn (callbale): data normalization function \n",
    "#             (can be specific for each pretrained neural network)\n",
    "#     Return:\n",
    "#         transform: albumentations.Compose\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     _transform = [\n",
    "#         A.Lambda(image=preprocessing_fn),\n",
    "#     ]\n",
    "#     return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zSQc2MKuwQH",
    "outputId": "e4c53a77-f9f5-4562-8033-35b08b770d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "VJ9BvVn4uwQI"
   },
   "outputs": [],
   "source": [
    "CLASSES =  ['roads','buildings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "oT3QPvOTuwQI"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset_multi(\n",
    "    BnR_train_input_list, \n",
    "    Bnr_train_label_list, bnR_train_label_list,\n",
    "    classes=CLASSES,\n",
    "    augmentation=augmentations(),\n",
    "    #preprocessing=get_preprocessing(preprocess_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bnR_train_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "mU1gzeN2uwQK"
   },
   "outputs": [],
   "source": [
    "valid_dataset = Dataset_multi(\n",
    "    BnR_val_input_list, \n",
    "    Bnr_val_label_list, bnR_val_label_list, \n",
    "    classes=CLASSES,    \n",
    "    augmentation=augmentations,\n",
    "   # preprocessing=get_preprocessing(preprocess_input)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djLHpAMmuwQK"
   },
   "source": [
    "\n",
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "mpuGrNNiuwQK"
   },
   "outputs": [],
   "source": [
    "BACKBONE = 'efficientnetb3'\n",
    "BATCH_SIZE = 1\n",
    "LR = 0.0001\n",
    "EPOCHS = 30\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "gXDWlHo5uwQL",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation\n",
    "activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "\n",
    "#create model\n",
    "model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8MzHY0OyvXO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "xDVgNPMsuwQL"
   },
   "outputs": [],
   "source": [
    "# define optomizer\n",
    "optim = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "# Segmentation models losses can be combined together by '+' and scaled by integer or float factor\n",
    "# set class weights for dice_loss (car: 1.; pedestrian: 2.; background: 0.5;)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=np.array([1, 2, 0.5])) \n",
    "focal_loss = sm.losses.BinaryFocalLoss() if n_classes == 1 else sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "# actulally total_loss can be imported directly from library, above example just show you how to manipulate with losses\n",
    "# total_loss = sm.losses.binary_focal_dice_loss # or sm.losses.categorical_focal_dice_loss \n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "\n",
    "# compile keras model with defined optimozer, loss and metrics\n",
    "model.compile(optim, total_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "kzRFQjZAuwQL"
   },
   "outputs": [],
   "source": [
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_dataloader = Dataloder(valid_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "BiJjNLN6uwQL"
   },
   "outputs": [],
   "source": [
    "# assert train_dataloader[0][0].shape == (BATCH_SIZE, 1024, 1024, 3)\n",
    "# assert train_dataloader[0][1].shape == (BATCH_SIZE, 1024, 1024, n_classes)\n",
    "\n",
    "# define callbacks for learning rate scheduling and best checkpoints saving\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('aug_test.h5', save_weights_only=True, save_best_only=True, mode='min'),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ER7ytco2uwQL",
    "outputId": "170898f0-cd86-4ccf-fcb5-f05fd69a6054"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.summary of <tensorflow.python.keras.engine.functional.Functional object at 0x7f147c4c70d0>>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "JfPDpNrguwQM",
    "outputId": "9d66461a-037f-4e89-8aea-8d5300635f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "115/324 [=========>....................] - ETA: 22:41 - loss: 0.5504 - iou_score: 0.3922 - f1-score: 0.5217"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-89f224bb923f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m      \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(\n",
    "    train_dataloader, \n",
    "    steps_per_epoch=len(train_dataloader), \n",
    "    epochs=30, \n",
    "     callbacks=callbacks,      validation_data=valid_dataloader, \n",
    "    validation_steps=len(valid_dataloader) ,verbose = 1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUUP8hLzuwQM"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation iou_score values\n",
    "plt.figure(figsize=(30, 8))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['iou_score'])\n",
    "plt.plot(history.history['val_iou_score'])\n",
    "plt.title('Model iou_score')\n",
    "plt.ylabel('iou_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irrSiMRiy-wi"
   },
   "source": [
    "## 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX0arTz-uwQM"
   },
   "outputs": [],
   "source": [
    "test_dataset = Dataset_multi(\n",
    "    BnR_val_input_list, \n",
    "    Bnr_val_label_list, bnR_val_label_list, \n",
    "    classes=CLASSES,    \n",
    "#     augmentation=get_validation_augmentation(),\n",
    "#         preprocessing=get_preprocessing(preprocess_input)\n",
    "\n",
    ")\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPWfv8oYuwQM"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load best weights\n",
    "model.load_weights('aug_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5bctPgCuwQN"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_dataloader)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))\n",
    "for metric, value in zip(metrics, scores[1:]):\n",
    "    print(\"mean {}: {:.5}\".format(metric.__name__, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cHUFtPruwQN"
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
    "\n",
    "for i in ids:\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image)\n",
    "    \n",
    "    visualize(\n",
    "        image=denormalize(image.squeeze()),\n",
    "        gt_mask=gt_mask.squeeze(),\n",
    "        pr_mask=pr_mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qtJ4_otuwQN"
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "ids = np.random.choice(np.arange(len(test_dataset)), size=n)\n",
    "\n",
    "for i in ids:\n",
    "    \n",
    "    image, gt_mask = test_dataset[i]\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    pr_mask = model.predict(image)\n",
    "    \n",
    "    visualize(\n",
    "        image=denormalize(image.squeeze()),\n",
    "        gt_mask=gt_mask.squeeze(),\n",
    "        pr_mask=pr_mask.squeeze(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F6Zxc4dyprf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "o6QP21LkuwQC"
   ],
   "name": "multi_bnr_augmentation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
